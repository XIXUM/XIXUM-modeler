model:
  transformer_model: "microsoft/deberta-v3-large"

optimizer:
  lr: 1.0e-05
  warmup_steps: 5000
  total_steps: ${training.trainer.max_steps}
  total_reset: 1
  weight_decay: 0.01
  lr_decay: 0.9
  no_decay_params:
    - "bias"
    - LayerNorm.weight

entities_per_forward: 75
relations_per_forward: 25
